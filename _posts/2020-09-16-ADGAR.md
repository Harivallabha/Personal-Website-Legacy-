---
title: 'Paper Analysis #1: ADGAR'
date: 2020-09-16
permalink: /posts/2020/09/ADGAR/
tags:
  - Adjoint GPU
---

## A GPU Accelerated Adjoint Solver for Shape Optimization in Viscous Flows

_Mishra_, _Jude_ and _Baeder_ presented the ADGAR solver at the _AIAA SciTech Forum, San Diego, 2019_. The scope of the paper is similar to the problem statement I'm working on for my first thesis at BITS Pilani - Hyderabad, so naturally it piqued my interest. This version extends the capability of the solver by adopting a purely automatic differentiation based pipeline for the computation of accurate adjoint sensitivities, whereas the previous version of ADGAR made use of hand-discrete adjoints. Here's a short analysis of the paper, sprinkled with some comments and takeaways.

- Goal: To develop a GPU accelerated discrete adjoint based optimization framework.
- What did they previously have? 
    - A GPU accelerated hand derived discrete adjoint solver for the 2-D Euler Case.

Hand-discrete adjoints aren't as elegant as AD-based adjoints, and suffer from minor accuracy issues. Moreover, maintaining, tweaking and debugging the hand discrete adjoints requires the future programmer to be well versed with such derivation procedures. For this reason, many popular aerodynamic shape optimization solvers make use of automatic differentiation tools such as CoDiPack (operator overloading), Tapenade (source transformation), etc. In our case, for example, I wrote the primal [meshfree solver](https://github.com/Scientific-Computing-BPHC/Meshfree_cpp) for inviscid compressible flows, in C++. And we intend to make use of [CoDiPack](https://www.scicomp.uni-kl.de/software/codi/) to generate the adjoints. We would then need to optimize the CoDiPack AD pipeline, and parallelize it with CUDA. In the FORTRAN version of the solver, we've generated the adjoint code using Tapenade. And now, we're in the process of GPU parallelizing the black-box generated AD code. This poses some difficulties, mainly due to the memory issues on the GPU. To overcome the memory issues, we plan to use a checkpointing procedure. Of course, this entails a memory-speed tradeoff but this is a tradeoff we're willing to go for because as far as GPUs are concerned computation is cheap, buddy, but memory is a very expensive resource.

_Small plug: CoDiPack is an operator overloading based automatic differentiation pipeline developed and maintained by our collaborators at TU - Kaiserlsautern, and used by popular multiphysics simulations tools such as [SU2](https://su2code.github.io/), Stanford. If you have C++ code for which you want AD based adjoints, and you're not using CoDiPack yet, you should seriously consider it!_

 Anyway, coming back to the paper:

- Contributions presented in the paper kya hai?
    - A GPU accelerated AD based adjoint solver for the 2-D Euler Case.
    - Gradient sensitivities using both the AD and hand-derived formula validated with finite-difference values.
    - Effectively optimized a NACA0012 airfoil shape to a target NACA2312 airfoil pressure distribution over only 5 (using SLSQP) to 7 (using CG) design iterations, using the adjoint solver.

Now, these are some exciting results. Firstly, they've adopted the more elegant AD based approach for sensitivity computations. And then, they've succesfully leveraged the power of the GPU to gain exciting speedups. Finally, the adjoint solver was able to effectively optimize the NACA0012 to the target NACA2312 within very few iterations of the backward solver. This is similar to what we want to do with our solver, and so all of these results are, personally, thrilling for me.

- One key difference is that we have a q-LSQUM based Meshfree solver, not a mesh-based solver as is the case of ADGAR. ADGAR is the adjoint extension of GARFIELD: a RANS solver for the three-dimensional, unsteady, compressible Navier-Stokes equations.  The solution evolves using implicit time integration. For the implicit operator, GARFIELD uses a Diagonalized Alternating Direction Implicit (DADI) operator with up-wind dissipation. The solver is computationally efficient, and makes use of both Point as well as Line Parallelism on the GPU. 
- Section A. of the paper discusses the Forward formulation. 
- Section B. discusses the Discrete Adjoint formulation.

_A small digression: I find it really fascinating that backprop was invented by the ML community, as well as the scientific computing community independently, and has served to be such an important cog in the optimization pipeline of both. Griewank has written some brilliant literature on the topic, and much of the ideas are yet to be ported over to the popular DL frameworks (TensorFlow, PyTorch, etc.). Right now, we also see the growth of Probabilistic Programming as a paradigm. A lot of work on this is going on in places like MSR Cambridge, where they're exploring Message Passing as a generalization of AD. I'd love to see such language level developments democratize the process of writing adjoint code for scientific computing applications._

- Section C. details the gradient based optimization procedure. The adjoint gradient sensitivity values are utilized to perform airfoil design optimization using the SciPy optimization library with both the Sequential Least Squares Programming (SLSQP) and Conjugate Gradient (CG) optimization methods. The airfoil geometry parameterization is achieved using “Hicks-Henne Bump Functions”.

- Grid generation is performed by solving a 2-D Poisson equation. Here, again we have a divergence. In our solver, we employ a Quadtree based approach for the generation of meshfree point clouds. 

- The rest of the paper elaborates on the specifics of the parallelization procedure employed in ADGAR, followed by a results section.

## Results

We can see that the AD based adjoint sensitivities are more accurate than the the hand-derived counterparts.

![image1](https://github.com/Harivallabha/harivallabha.github.io/tree/master/images/sens.png)

And, here's a table of the speedup achieved. Makes me love GPUs, oh!

![image2](https://github.com/Harivallabha/harivallabha.github.io/tree/master/images/speedup.png)

Comparison of the convergence on the CPU against the GPU; Laminar Solutions, Euler Residuals, and Adjoint Solutions:

![image3](https://github.com/Harivallabha/harivallabha.github.io/tree/master/images/gpu-prim.png)

![image4](https://github.com/Harivallabha/harivallabha.github.io/tree/master/images/adj.png)

Optimization, with target solutions, using the SLSQP (for six variables). Shows that the effectivness of the adjoint approach implemented!

![image5](https://github.com/Harivallabha/harivallabha.github.io/tree/master/images/adjoint-final.png)