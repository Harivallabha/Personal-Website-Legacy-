---
title: 'Paper Analysis #1: ADGAR'
date: 2020-09-14
permalink: /posts/2020/09/ADGAR/
tags:
  - Adjoint GPU
---

## A GPU Accelerated Adjoint Solver for Shape Optimization in Viscous Flows

Mishra, Jude and Baeder presented the ADGAR solver at the AIAA SciTech Forum, San Diego, 2019. The scope of the paper is similar to the problem statement I'm working on for my first thesis at BITS Pilani - Hyderabad, and so naturally it piqued my interest. This version extends the capability of the solver by adopting a purely automatic differentiation based pipeline for the computation of accurate adjoint sensitivities, whereas the previous version of ADGAR made use of hand-discrete adjoints. Here's a short analysis of the paper, sprinkled with some comments and takeaways.

- Goal: To develop a GPU Accelerated Discrete Adjoint based optimization framework.
- What did they previously have? 
    - A GPU Accelerated hand derived discrete adjoint solver for the 2-D Euler Case.

Handle discrete adjoints aren't as elegant as AD-based adjoints, and suffer from minor accuracy issues. Moreover, maintaining, tweaking and debugging the hand discrete adjoints requires the future programmer to be well versed with such derivation procdures. For this reason, many popular aerodynamic shape optimization solvers make use of automatic differentiation tools such as CoDiPack (operator overloading), Tapenade (source transformation), etc. In our case, for example, I wrote the primal meshfree solver for invscid compressible flows, in C++. and we intend to make use of CoDiPack to generate the adjoints. We would then need to optimize the CoDiPack AD pipeline, and CUDAfy it. In the FORTRAN version of the solver, we generated the adjoint code using Tapenade. And now, we're in the process of GPU parallelizing the black-box generated AD code. This poses some difficulties, mainly due to the memory issues on the GPU. To overcome the memory issues, we plan to use a checkpointing procedure. Of course, this entails a memory-speed tradeoff but this is a tradeoff we're willing to go for because as far as GPUs are concerned computation is cheap, buddy, but memory is a very expensive resource. Anyway, coming back to the paper:

- What's the addition to their solver presented in this paper?
    - A GPU Accelerated AD based adjoint solver for the 2-D Euler Case.
    - Gradient sensitivities using both the AD and hand-derived formula were validated with finite-difference values.
    - Using the adjoint solver, effectively optimized a NACA0012 airfoil shape to a target NACA2312 airfoil pressure distribution over only 5 (using SLSQP) to 7 (using CG) design iterations.

Now, these are some exciting results. Firstly, they moved to the more elegant AD based approach for sensitivity computation. And then, they've succesfully leveraged the power of the GPU to gain exciting speedups. Finally, the adjoint solver was able to effectively optimize the NACA0012 to the target NACA2312 within very few iterations of the backward solver. This is similar to what we want to do with our solver, and so all of these results are, personally, thrilling for me.

- One key difference is that we have a q-LSQUM based Meshfree solver, not a Mesh based solver as is the case of ADGAR. ADGAR is the adjoint extension of GARFIELD: a RANS solver for the three-dimensional, unsteady, compressible Navier-Stokes equations.  The solution evolves using implicit time integration. For the implicit operator, GARFIELD uses a Diagonalized Alternating Direction Implicit (DADI) operator with up-wind dissipation. the solver is computationally efficient, and makes use of both point as well as line parallelism on the GPU. Section A. of the paper discusses the Forward formulation. Section B. discusses the Discrete Adjoint Formulation.

A small digression: I find it really fascinating that backprop was invented by the ML community, as well as the scientific computing community independently, and has served to be such an important cog in the optimization pipeline of both. Griewank has written some brilliant literature on the topic, and much of the ideas are yet to be ported over to the popular DL frameworks (TensorFlow, PyTorch, etc.) Right now, we also see the growth of Probabilistic Programming as a paradigm. A lot of work on this is going on in places like MSR Cambridge, where they're exploring Message Passing as a generalization of AD. I'd love to see such language level developments democratize the process of writing adjoint code for scientific computing applications.

- Section C. details the gradient based optimization procedure. The adjoint gradient sensitivity values are utilized to perform airfoil design optimization using SciPy optimiza-tion library with both the Sequential Least Squares Programming (SLSQP) and Conjugate Gradient (CG) optimization methods. The airfoil geometry parameterization is achieved using “Hicks-Henne Bump Functions”.

- Grid generation is performed by solving a 2-D Poisson equation. Here, again we have a divergence. In our solver, we employ a Quadtree based approach for the generation of meshfree point clouds.

The rest of the paper elaborates on the specifics of the parallelization they achieved using the GPU in ADGAR, followed by a results section.

